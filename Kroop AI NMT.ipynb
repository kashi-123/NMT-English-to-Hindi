{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.30.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences= pd.read_json(r'C:\\Users\\Asus\\Desktop\\Training\\trainEnglish.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrea Masi opened the scoring in the fourth m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despite controlling the game for much of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portugal never gave up and David Penalva score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Italy led 16-5 at half time but were matched b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>However Bortolussi scored his fourth penalty o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading\n",
       "0  Andrea Masi opened the scoring in the fourth m...\n",
       "1  Despite controlling the game for much of the f...\n",
       "2  Portugal never gave up and David Penalva score...\n",
       "3  Italy led 16-5 at half time but were matched b...\n",
       "4  However Bortolussi scored his fourth penalty o..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences=english_sentences[['Heading']]\n",
    "english_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20105, 1)\n"
     ]
    }
   ],
   "source": [
    "print(english_sentences.shape)\n",
    "english_sentences= english_sentences['Heading'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>एंड्रिया मैसी ने चौथे मिनट में इटली के लिए ट्र...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>पहले हाफ़ के अधिकांश समय तक खेल नियंत्रण में र...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>हार न मानते हुए, पुर्तगाल के डेविड पेनलावा ने ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>पहले हाफ़ में इटली 16-5 से आगे था पर दूसरे हाफ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>हालाँकि बोर्तोलुसी ने मैच का चौथा पेनाल्टी स्क...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading\n",
       "0  एंड्रिया मैसी ने चौथे मिनट में इटली के लिए ट्र...\n",
       "1  पहले हाफ़ के अधिकांश समय तक खेल नियंत्रण में र...\n",
       "2  हार न मानते हुए, पुर्तगाल के डेविड पेनलावा ने ...\n",
       "3  पहले हाफ़ में इटली 16-5 से आगे था पर दूसरे हाफ...\n",
       "4  हालाँकि बोर्तोलुसी ने मैच का चौथा पेनाल्टी स्क..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sentences= pd.read_json(r'C:\\Users\\Asus\\Desktop\\Training\\trainHindi.json') \n",
    "hindi_sentences= hindi_sentences[['Heading']]  \n",
    "hindi_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>एंड्रिया मैसी ने चौथे मिनट में इटली के लिए ट्र...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>पहले हाफ़ के अधिकांश समय तक खेल नियंत्रण में र...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>हार न मानते हुए, पुर्तगाल के डेविड पेनलावा ने ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>पहले हाफ़ में इटली 16-5 से आगे था पर दूसरे हाफ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>हालाँकि बोर्तोलुसी ने मैच का चौथा पेनाल्टी स्क...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading\n",
       "0  एंड्रिया मैसी ने चौथे मिनट में इटली के लिए ट्र...\n",
       "1  पहले हाफ़ के अधिकांश समय तक खेल नियंत्रण में र...\n",
       "2  हार न मानते हुए, पुर्तगाल के डेविड पेनलावा ने ...\n",
       "3  पहले हाफ़ में इटली 16-5 से आगे था पर दूसरे हाफ...\n",
       "4  हालाँकि बोर्तोलुसी ने मैच का चौथा पेनाल्टी स्क..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20105, 1)\n"
     ]
    }
   ],
   "source": [
    "print(hindi_sentences.shape)\n",
    "hindi_sentences= hindi_sentences['Heading'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before going fprward and apply encoder and decoder, data needs to be preprocessed.\n",
    "In this dataset there may be sentences with empty string, extra spaces, uppercase letters etc.\n",
    "\n",
    "For preprocessing the data, we will first take care of the extra spaces, replace eveything with space except a-z, A-Z,'?','.','!',','.\n",
    "we will add 'sentencestart' & 'sentenceend' at the front and back respectively, so that the model knows when does the sentence end and start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, english):\n",
    "    if (type(sen) != str):\n",
    "        return ''\n",
    "    \n",
    "    sentence = sentence.strip('.')   #remove the '.' in the sentence\n",
    "    \n",
    "    #The re.sub() function is used to replace occurrences of a particular sub-string with another sub-string. \n",
    "    #This function takes as input the following:\n",
    "    #The sub-string to replace, The sub-string to replace with, The actual string\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,¿;।])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    #now, Replacing everything except alphabets (upper & lower case both), \",\", \".\",\"?\",\"!\".\"'\"\n",
    "    if(english == True):\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sentence)\n",
    "        sentence = sentence.lower()      #changing all uppercases to lowercase\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    sentence = 'sentencestart ' + sentence + ' sentenceend'\n",
    " \n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencestart i am ? a gil . . who is not sentenceend\n"
     ]
    }
   ],
   "source": [
    "#just checking if my function works\n",
    "sen = \"I am ? a /// gil .. who is NOT.\"\n",
    "t = preprocess_sentence(sen,True)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets set maximum number of words in a sentence\n",
    "num_words = 10000\n",
    "MAX_WORDS_IN_A_SENTENCE = 100\n",
    "english_vocab_size = num_words + 1\n",
    "hindi_vocab_size = num_words + 1\n",
    "test_ratio = 0.2\n",
    "embedding_dim = 64\n",
    "hidden_units = 1024\n",
    "learning_rate = 0.006\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20105\n"
     ]
    }
   ],
   "source": [
    "print(len(english_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going  through all data points to make sure that no empty strings are considerend and no string has values more than MAX_WORDS_IN_A_SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_e_sentences = []\n",
    "processed_h_sentences = []\n",
    "\n",
    "for i in range(len(english_sentences)):\n",
    "    s = preprocess_sentence(english_sentences[i],True)\n",
    "    processed_e_sentences.append(s)\n",
    "\n",
    "    \n",
    "for i in range(len(hindi_sentences)):\n",
    "    s = preprocess_sentence(hindi_sentences[i],False)\n",
    "    processed_h_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20105"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exploring and checking what tweaks we made above\n",
    "\n",
    "len(processed_h_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now vetorize the text corpus.\n",
    "fit_on_texts() assigns a unique index to each word. \n",
    "texts_to_Sequence() convert a text sentence to a list of numbers or a vector where the number points to the unique index of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use pad_sequences() to make sure that all of the vetors are of same length, by appending oov_token (out-of-vocab) just enough number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "oov_token = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(processed, num_words, oov_token):\n",
    "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
    "    tokenizer.fit_on_texts(processed)\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(processed)\n",
    "    sequences = pad_sequences(sequences, padding = 'post')\n",
    "    return word_index, sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 1, 'e': 2, 't': 3, 'n': 4, 'a': 5, 'r': 6, 'i': 7, 's': 8, 'o': 9, 'h': 10, 'c': 11, 'd': 12, 'm': 13, 'f': 14, 'u': 15, 'y': 16, 'p': 17, 'g': 18, 'w': 19, 'l': 20}\n"
     ]
    }
   ],
   "source": [
    "i,s,t = tokenize_sentences(processed_e_sentences[0], num_words, oov_token)\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencestart andrea masi opened the scoring in the fourth minute with a try for italy sentenceend\n"
     ]
    }
   ],
   "source": [
    "print(processed_e_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_index, english_sequences, english_tokenizer = tokenize_sentences(processed_e_sentences, num_words, oov_token)\n",
    "hindi_word_index, hindi_sequences, hindi_tokenizer = tokenize_sentences(processed_h_sentences, num_words, oov_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data set into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_sequences, english_val_sequences, hindi_train_sequences, hindi_val_sequences = train_test_split(english_sequences, hindi_sequences, test_size = test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(english_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16084\n"
     ]
    }
   ],
   "source": [
    "size = len(english_train_sequences)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divind the dataset into batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we define Batch size:\n",
    "The batch size is a number of samples processed before the model is updated. The number of epochs is the number of complete passes through the training dataset. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the help of tf.data.Dataset.from_tensor_slices() method, we can get the slices of an array in the form of objects\n",
    "dataset = tf.data.Dataset.from_tensor_slices((english_train_sequences, hindi_train_sequences)).shuffle(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# as_numpy_iterator Returns an iterator which converts all elements of the dataset to numpy.\n",
    "\n",
    "print(len(list(dataset.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Seq2Seq architecture and have followed this tutorial https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it involves 2 LSTM (Long short term memory), one for encoding and other for decoding. But, I would rather use GRU (Gated Recurrent Units) for both encoding and decoding since it takes less computation power, and we can achieve similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) each word in the input sentence is embedded and are represented in a different space embedding_dim (hyperparamter) dimensions. Hence, words like (girl, woman), (Book, Novel) are located closer in this space. So, the word \"girl\" has a chance of getting predicted same as chance of \"woman\", since they hold similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Now this embedded sentence is passed to GRU. The final hidden state of encoder GRU becomes the maiden state for decoder GRU. This final hidden state of encoder GRU, consists of encoding or information about the original/ source sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, english_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(english_vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(hidden_units, return_sequences = True, return_state = True)\n",
    "    \n",
    "    def call(self, input_sequence):\n",
    "        x = self.embedding(input_sequence)\n",
    "        encoder_sequence_output, final_encoder_state = self.gru(x)\n",
    "        return encoder_sequence_output, final_encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize encoder\n",
    "\n",
    "encoder = Encoder(english_vocab_size, embedding_dim, hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder (without Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the target sentence is generated by the decoder GRU network language model. As discussed before, the final encoding hidden state is used as the initial hidden state for the decoder GRU. \n",
    "'sentencestart' is given to the GRU as the first input, to predict the next. This token is used to predict probability of occuerance of all num_words (10000) number of words. The word with maximum probabilty becomes the next input of GRU, and it is all repeated will we reach sentenceend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with this approach:\n",
    "\n",
    "Since the encoder's final state becomes the initial hidden state for the decoder,so all the information of the information of the source has to be compressed in the final state. It might be biased to the information at the end of the sentence. And this is an issue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to solve this, encoder final hidden state is not just taken into consideration, WEIGHTED SUM of all the outputs from the encoder is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to solve it, I used ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention literaaly gives weight to each word in a sentence. When there is a need to know which encoder output holds the similar informationn as in the decoder hidden state Attention is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector of Attention Scores are calculated at each step when a word is being predicted at the GRU cell in the decoder. This vector determines the weightage of each encoder output to find the weighted sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed-length vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the \"Basic Dot Product Attention\". The name is quite self-explanatory, it is the dot product of the input matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic dot product attention has an assumption. According to it, the dimensions of both the input matrix on the axis where the dot product is to be taken, needs to be same. This dimention in my implemnetation is given by hidden_units, which os same for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(BasicDotProductAttention, self).__init__()\n",
    "\n",
    "    def call(self, decoder_hidden_state, encoder_outputs):\n",
    "       #Dimensions of decoder_hidden_state => (BATCH_SIZE, hidden_units)\n",
    "       #Dimensions of encoder_outputs => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, hidden_units)\n",
    "\n",
    "        decoder_hidden_state_with_time_axis = tf.expand_dims(decoder_hidden_state, 2)\n",
    "        #Dimensions of decoder_hidden_state_with_time_axis => (BATCH_SIZE, hidden_units, 1)\n",
    "        attention_scores = tf.matmul(encoder_outputs, decoder_hidden_state_with_time_axis)\n",
    "        #Dimensions of attention_scores => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, 1)\n",
    "        attention_scores = tf.nn.softmax(attention_scores, axis = 1)\n",
    "        weighted_sum_of_encoder_outputs = tf.reduce_sum(encoder_outputs * attention_scores, axis = 1)\n",
    "        #Dimensions of weighted_sum_of_encoder_outputs => (BATCH_SIZE, hidden_units)\n",
    "\n",
    "        return weighted_sum_of_encoder_outputs, attention_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder (with attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in Decoder class:\n",
    "\n",
    "like in encoder, here is also an embedding layer for sequences in the to-be-converted-into language, here it is Hindi. each word in a sequence is represented in the embedding space with similar meaning words are placed closer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after calling the attnetion layer, weighted sum of encoder outputs is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result (representation of embedding space and weighted sum of encoder output), is abtained and is sent to the GRU layer of the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of the GRU layer is then sent to the dense layer which gives the probability of  occurrence of all of the hindi_vocab_size number of words. Word with high probability implies that the model thinks that this word should be the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, hindi_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(hindi_vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(hidden_units, return_state = True)\n",
    "        self.word_probability_layer = tf.keras.layers.Dense(hindi_vocab_size, activation = 'softmax')\n",
    "        self.attention_layer = BasicDotProductAttention()\n",
    "        \n",
    "    def call(self, decoder_input, decoder_hidden, encoder_sequence_output):\n",
    "        x = self.embedding(decoder_input)\n",
    "        \n",
    "        \n",
    "        weighted_sum_of_encoder_outputs, attention_scores = self.attention_layer(decoder_hidden, encoder_sequence_output)\n",
    "        \n",
    "        \n",
    "        x = tf.concat([weighted_sum_of_encoder_outputs, x], axis = -1)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "        \n",
    "        \n",
    "        decoder_output, decoder_state = self.gru(x)\n",
    "        \n",
    "        \n",
    "        word_probability = self.word_probability_layer(decoder_output)\n",
    "        \n",
    "        \n",
    "        return word_probability, decoder_state, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the decoder\n",
    "decoder = Decoder(hindi_vocab_size, embedding_dim, hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process till now...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) find the encoder output and encoder hidden state from the encoder oject. these outputs is used to find the attention score. the encoder final hidden state is the initial hidden state of decoder\n",
    "\n",
    "2) to predict the next word in hindi, an input word, previous decoder hidden state, encoder sequence outputs are given to the decoder object. word prediction probabilty and current hidden state are returned\n",
    "\n",
    "3) maximum probabilty word is considered as the input for the next decoder GRU cell. the current decoder hidden state becomes the input hudden state for the next decoder GRU cell.\n",
    "\n",
    "4) loss is calculated using the word prediction probability and actual word in the target sentence\n",
    "\n",
    "in every epoch, these steps are called for all the batches & loss for all the epochs is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse categorical crossentropy loss and adam optimizer are chosen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "def loss_function(actual_words, predicted_words_probability):\n",
    "    loss = loss_object(actual_words, predicted_words_probability)\n",
    "    mask = tf.where(actual_words > 0, 1.0, 0.0)\n",
    "    return tf.reduce_mean(mask * loss)\n",
    "\n",
    "\n",
    "def train_step(english_sequences, hindi_sequences):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_sequence_output, encoder_hidden = encoder(english_sequences)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = hindi_sequences[:, 0]\n",
    "        for i in range(1, hindi_sequences.shape[1]):\n",
    "            predicted_words_probability, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_sequence_output)\n",
    "            actual_words = hindi_sequences[:, i]\n",
    "            \n",
    "            if np.count_nonzero(actual_words) == 0:\n",
    "                break\n",
    "            loss += loss_function(actual_words, predicted_words_probability)\n",
    "\n",
    "            decoder_input = actual_words\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss.numpy()\n",
    "all_epoch_losses = []\n",
    "training_start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No.: 0 Time: 1273.6945250034332\n",
      "Epoch No.: 1 Time: 1613.1667745113373\n",
      "Epoch No.: 2 Time: 1539.1818432807922\n",
      "Epoch No.: 3 Time: 957.1024177074432\n",
      "Epoch No.: 4 Time: 955.8483700752258\n",
      "Epoch No.: 5 Time: 950.5274212360382\n",
      "Epoch No.: 6 Time: 952.3498854637146\n",
      "Epoch No.: 7 Time: 955.741516828537\n",
      "Epoch No.: 8 Time: 952.1819429397583\n",
      "Epoch No.: 9 Time: 951.3708064556122\n",
      "Epoch No.: 10 Time: 954.2991886138916\n",
      "Epoch No.: 11 Time: 951.8446118831635\n",
      "Epoch No.: 12 Time: 951.538083076477\n",
      "Epoch No.: 13 Time: 952.9957630634308\n",
      "Epoch No.: 14 Time: 949.7646601200104\n",
      "Epoch No.: 15 Time: 951.6074774265289\n",
      "Epoch No.: 16 Time: 946.1992561817169\n",
      "Epoch No.: 17 Time: 952.9822399616241\n",
      "Epoch No.: 18 Time: 950.4610857963562\n",
      "Epoch No.: 19 Time: 4120.194446802139\n",
      "Epoch No.: 20 Time: 1456.2308254241943\n",
      "Epoch No.: 21 Time: 1368.2881054878235\n",
      "Epoch No.: 22 Time: 1749.5814476013184\n",
      "Epoch No.: 23 Time: 12891.44866681099\n",
      "Epoch No.: 24 Time: 1302.8859837055206\n",
      "Epoch No.: 25 Time: 1091.2967038154602\n",
      "Epoch No.: 26 Time: 1080.2339227199554\n",
      "Epoch No.: 27 Time: 1045.0028986930847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-2a58d1698b2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menglish_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhindi_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhindi_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-ea62aab922b4>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(english_sequences, hindi_sequences)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1731\u001b[0m   \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1733\u001b[1;33m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1734\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5692\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5693\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5694\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5695\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5696\u001b[0m         transpose_b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    start_time = time.time()\n",
    "    for(batch, (english_sequences, hindi_sequences)) in enumerate(dataset):\n",
    "        batch_loss = train_step(english_sequences, hindi_sequences)\n",
    "        epoch_loss.append(batch_loss)\n",
    "    \n",
    "    all_epoch_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    print(\"Epoch No.: \" + str(epoch) + \" Time: \" + str(time.time()-start_time))\n",
    "    \n",
    "print(\"All Epoch Losses: \" + str(all_epoch_losses))\n",
    "print(\"Total time in training: \" + str(time.time() - training_start_time))\n",
    "\n",
    "plt.plot(all_epoch_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Epoch Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained for 12 hours+, but still the training was not complete, so I interupter the kernal, so that I can move forward with the testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) preprocess the input i.e the sentence which is to be translated\n",
    "\n",
    "2) so now the above ouput is fed to trained encoder which return the encoded sequence output, and encoder's final hidden state.\n",
    "\n",
    "3) eencoder's final state is the decoder's first hidden state and the first input to the decoder is a start token \"sentencestart\"\n",
    "\n",
    "4) decoder return the predicted word probabilities. Maximum probability word is appened to the required final hindi sentence.\n",
    "\n",
    "5) it goes on till we reach \"sentenceend\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_from_sequences(sequences, tokenizer):\n",
    "    return tokenizer.sequences_to_texts(sequences)\n",
    "\n",
    "# Testing start!\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    sentence = preprocess_sentence(sentence, True)\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])[0]\n",
    "    sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=MAX_WORDS_IN_A_SENTENCE, padding='post')\n",
    "    encoder_input = tf.convert_to_tensor(sequence)\n",
    "    encoder_sequence_output, encoder_hidden = encoder(encoder_input)\n",
    "    decoder_input = tf.convert_to_tensor([hindi_word_index['sentencestart']])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    sentence_end_word_id = hindi_word_index['sentenceend']\n",
    "    hindi_sequence = []\n",
    "    for i in range(MAX_WORDS_IN_A_SENTENCE*2):\n",
    "        predicted_words_probability, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_sequence_output)\n",
    "        #taking the word with maximum probability\n",
    "        predicted_word_id = tf.argmax(predicted_words_probability[0]).numpy()\n",
    "        hindi_sequence.append(predicted_word_id)\n",
    "        #if the word 'sentenceend' is predicted, exit the loop\n",
    "        if predicted_word_id == sentence_end_word_id:\n",
    "            break\n",
    "        decoder_input = tf.convert_to_tensor([predicted_word_id])\n",
    "    print(sentence)\n",
    "    return get_sentence_from_sequences([hindi_sequence], hindi_tokenizer)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencestart they said that this is not possible sentenceend\n",
      "['उन्होंने कहा कि यह नहीं है । sentenceend']\n"
     ]
    }
   ],
   "source": [
    "print(translate_sentence(\"They said that this is not possible\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
